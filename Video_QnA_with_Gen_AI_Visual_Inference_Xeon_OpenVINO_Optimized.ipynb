{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "073fd077-2cad-449f-9268-48417e69ee18",
      "metadata": {
        "id": "073fd077-2cad-449f-9268-48417e69ee18"
      },
      "source": [
        "# Description:\n",
        "This notebook focuses on video analysis using Generative AI. It begins by extracting audio from a video, transcribing it into text, and then applying AI techniques to query or summarize the transcription, offering quick insights. The video is also broken down into frames at a configurable rate, allowing for flexible image generation. These frames are processed for visual question-answering, with redundant information removed, and a combined response is generated using a large language model (LLM). This approach enables effective interaction with video content to extract meaningful insights. This is also utilizing optimization using OpenVino"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbf5485b-2f34-4caa-a5b0-909cad451183",
      "metadata": {
        "id": "bbf5485b-2f34-4caa-a5b0-909cad451183"
      },
      "source": [
        "## Install necessary libraries\n",
        "These instructions are for setting up a Python environment with various libraries and tools, mainly for handling video processing, machine learning, and serving AI models. Here's a breakdown:\n",
        "\n",
        "- **opencv-python**: Installs OpenCV, a library for computer vision tasks.\n",
        "- **moviepy**: Installs MoviePy, a library for video editing.\n",
        "- **ffmpeg**: Installs FFmpeg, a tool for handling multimedia files.\n",
        "- **transformers**: Installs Hugging Face's Transformers library for working with transformer-based models.\n",
        "- **torch, torchvision, torchaudio**: Installs PyTorch and related libraries for machine learning tasks with images (torchvision) and audio (torchaudio).\n",
        "- **accelerate**: Installs Accelerate, a library for optimizing and distributing model training.\n",
        "- **pillow, flask**: Installs Pillow (image handling) and Flask (for creating web applications).\n",
        "- **txtai**: Installs the txtai library (with API and pipeline options), which enables AI-powered search and embeddings.\n",
        "\n",
        "These are used to set up an environment for video, image, and text processing with machine learning capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da139805-4e3f-4604-8467-9499d5aa7ea3",
      "metadata": {
        "scrolled": true,
        "id": "da139805-4e3f-4604-8467-9499d5aa7ea3"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python\n",
        "!pip3 install moviepy\n",
        "!sudo apt install ffmpeg -y\n",
        "!pip3 install transformers\n",
        "!pip3 install torch torchvision torchaudio\n",
        "!pip3 install accelerate -y\n",
        "!pip3 install torch pillow flask\n",
        "!pip install git+https://github.com/neuml/txtai#egg=txtai[api,pipeline]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1023fc6b-db60-4a53-aea6-58057e1c6594",
      "metadata": {
        "id": "1023fc6b-db60-4a53-aea6-58057e1c6594"
      },
      "source": [
        "## Hugging Face Login\n",
        "This code imports the `notebook_login` function from the Hugging Face Hub and calls it to prompt the user to enter their Hugging Face token. This token is necessary to authenticate the user and gain access to Hugging Face's resources, such as models and datasets, from within a Jupyter Notebook or similar environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f6c4bbc-d429-4f46-bafe-c43c28e7e7c2",
      "metadata": {
        "id": "4f6c4bbc-d429-4f46-bafe-c43c28e7e7c2"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# This will prompt you to enter your Hugging Face token\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a10c728a-5a80-4a20-997c-e1c3cd51b397",
      "metadata": {
        "id": "a10c728a-5a80-4a20-997c-e1c3cd51b397"
      },
      "source": [
        "## TXTAI directory setup\n",
        "This code sets up a directory named `txtai` by first ensuring that any pre-existing directory with that name is removed. It starts by defining the path to the directory as `\"txtai\"`, and then runs a shell command to check if the directory already exists. If it does, it deletes the directory along with all of its contents using the `sudo rm -rf` command. After clearing any previous data, the code creates a new directory named `txtai` using the `sudo mkdir -p` command, which ensures the directory is created without any errors, even if it already exists. This guarantees that a fresh, empty directory is available for use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2df324c-3885-4e16-aa85-a3cf14554231",
      "metadata": {
        "id": "b2df324c-3885-4e16-aa85-a3cf14554231"
      },
      "outputs": [],
      "source": [
        "# Define the directory path\n",
        "directory_path = \"txtai\"\n",
        "\n",
        "# Execute the shell commands\n",
        "!if [ -d \"{directory_path}\" ]; then sudo rm -rf \"{directory_path}\"; fi\n",
        "!sudo mkdir -p \"{directory_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eed0f54-ae08-455a-8bce-c9dd0ad34ed3",
      "metadata": {
        "id": "7eed0f54-ae08-455a-8bce-c9dd0ad34ed3"
      },
      "source": [
        "## TXTAI Pipeline\n",
        "This code initializes a transcription model by importing the `Transcription` class from the `txtai.pipeline` module, which is designed to convert audio into text. By calling `Transcription()`, it creates an instance named `transcribe`, which can now be used to handle transcription tasks. This allows the user to input audio files and convert spoken language into written text automatically. In summary, the code sets up a transcription model from the `txtai` library, enabling efficient audio-to-text conversion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c5a1899-8f2d-4ea6-88ab-440fbe4485be",
      "metadata": {
        "id": "3c5a1899-8f2d-4ea6-88ab-440fbe4485be"
      },
      "outputs": [],
      "source": [
        "from txtai.pipeline import Transcription\n",
        "\n",
        "# Create transcription model\n",
        "transcribe = Transcription()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1db38744-fdfa-46c2-b1ab-edc800c00716",
      "metadata": {
        "id": "1db38744-fdfa-46c2-b1ab-edc800c00716"
      },
      "source": [
        "\n",
        "## Generate audio from video\n",
        "This code handles video and audio processing by using the `moviepy.editor` module. It starts by importing the library as `mp` for video editing tasks. Next, it loads a video file called `your_video.mp4` into a `VideoFileClip` object, making it accessible for further manipulation. The code then extracts the audio track from the video, assigning the output to a file named `your_video.wav`. After successfully extracting and saving the audio, a shell command is executed to copy the `your_video.wav` file into the `txtai` directory using `sudo cp`. In summary, this code processes a video by extracting its audio, saving it in `.wav` format, and moving the audio file to a designated directory for further use.\n",
        "\n",
        "**Replace all references of `your_video` or `your_video.mp4` or `your_video.wav` with the actual name of your video file or video clip.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a254a27-8643-498a-a6ce-2a9f70a76a01",
      "metadata": {
        "id": "2a254a27-8643-498a-a6ce-2a9f70a76a01"
      },
      "outputs": [],
      "source": [
        "import moviepy.editor as mp\n",
        "\n",
        "# Load the video file\n",
        "video = mp.VideoFileClip(\"your_video.mp4\")\n",
        "\n",
        "# Extract audio from the video\n",
        "audio_path = \"your_video.wav\"\n",
        "video.audio.write_audiofile(audio_path)\n",
        "\n",
        "!sudo cp your_video.wav ./txtai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc11951-66ce-49b9-afc3-b6def5e6808f",
      "metadata": {
        "id": "cdc11951-66ce-49b9-afc3-b6def5e6808f"
      },
      "source": [
        "## List of audio files to process\n",
        "This code sets up the necessary imports and file list to display and play audio files within a Jupyter notebook. First, it imports the `Audio` and `display` functions from the `IPython.display` module, which are used to handle and showcase audio files. Then, it creates a list named `files` containing the file name `your_video.wav`. The second line modifies this list by prepending the path `txtai/` to each file name, resulting in a full file path like `txtai/your_video.wav`. In summary, this code prepares audio files for playback from the `txtai` directory and allows for easy extension by adding more audio files to the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f11e5c31-297f-4c9a-bbb3-b4f38642a96e",
      "metadata": {
        "id": "f11e5c31-297f-4c9a-bbb3-b4f38642a96e"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio, display\n",
        "\n",
        "files = [\"your_video.wav\"] # Specify here for multiple file\n",
        "files = [\"txtai/%s\" % x for x in files]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "865daba8-9225-4f73-9e8a-e29f1a3296e4",
      "metadata": {
        "id": "865daba8-9225-4f73-9e8a-e29f1a3296e4"
      },
      "source": [
        "## Transcription\n",
        "\n",
        "\n",
        "This code initializes a transcription model using OpenAI's Whisper model, specifically the `\"openai/whisper-base\"` version, which is designed for automatic speech recognition (ASR) tasks. The model, `transcribe`, is used to process audio files listed in the `files` variable (e.g., `\"txtai/your_video.wav\"`). The code iterates over the transcription results, where the `transcribe(files)` function processes each audio file and converts spoken content into text. Finally, the transcriptions are printed to the console using `print(text)`. In summary, this code leverages the Whisper model to transcribe audio files and display the transcribed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83de1470-edca-478b-9995-2047f0fc14cf",
      "metadata": {
        "id": "83de1470-edca-478b-9995-2047f0fc14cf"
      },
      "outputs": [],
      "source": [
        "# Transcribe files\n",
        "transcribe = Transcription(\"openai/whisper-base\")\n",
        "for text in transcribe(files):\n",
        "  print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d188f2b4-34ea-425d-b449-049560a4605e",
      "metadata": {
        "id": "d188f2b4-34ea-425d-b449-049560a4605e"
      },
      "source": [
        "## Ingestion directory setup\n",
        "This code below sets up the directory for document ingestion into a vector database. It first defines the directory path as `/home/ubuntu/documents` and checks if the directory already exists. If it does, the directory and its contents are deleted using the `rm -rf` command. Afterward, a new directory is created at the same location using the `mkdir -p` command. This ensures a clean directory structure is prepared for ingesting documents into the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b406d8f6-cab4-4467-9cb7-e16f708be1d9",
      "metadata": {
        "id": "b406d8f6-cab4-4467-9cb7-e16f708be1d9"
      },
      "outputs": [],
      "source": [
        "# Define the directory path for ingestion of documents into vector db\n",
        "ingestion_path = \"/home/ubuntu/documents\"\n",
        "\n",
        "# Execute the shell commands\n",
        "!if [ -d \"{ingestion_path}\" ]; then sudo rm -rf \"{ingestion_path}\"; fi\n",
        "!sudo mkdir -p \"{ingestion_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd92fc65-b6ae-4a96-aeb1-8d71970d6d7f",
      "metadata": {
        "id": "bd92fc65-b6ae-4a96-aeb1-8d71970d6d7f"
      },
      "source": [
        "## Chat with transcribed text using Gen AI LLM\n",
        "This code uses a pre-trained question-answering model from the `transformers` library to answer questions about video content. The `ask_question` function takes a question and uses the `qa_pipeline` to generate an answer based on a provided context (the transcribed video text). An example question, \"What happened to the 17 year old boy?\", is passed to the function, and the answer is printed. This allows users to interactively ask questions about a video using natural language processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eff6688-dc9c-4f6a-93a1-fa4adb416c5f",
      "metadata": {
        "id": "9eff6688-dc9c-4f6a-93a1-fa4adb416c5f"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "# Load a question-answering pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", device=-1)\n",
        "\n",
        "# Function to ask questions about the video\n",
        "def ask_question(question):\n",
        "    context = text\n",
        "    result = qa_pipeline(question=question, context=context, max_tokens=100)\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "question = \"What happened to the 17 year old boy?\"\n",
        "\n",
        "t1 = time.time()\n",
        "answer = ask_question(question)\n",
        "t2 = time.time()\n",
        "t3 = t2 - t1\n",
        "\n",
        "print(\"QnA from transcribed text takes: \", t3, \" seconds\")\n",
        "print(f\"Question: {question}\\nAnswer: {answer['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14684a7f-2e92-466e-8a65-74787446a5f2",
      "metadata": {
        "id": "14684a7f-2e92-466e-8a65-74787446a5f2"
      },
      "source": [
        "## Summarize transcribed text using Gen AI LLM\n",
        "This code utilizes the `transformers` library to generate a summary of a given document using a pre-trained summarization model, specifically `\"facebook/bart-large-cnn\"`. The summarization process is controlled by parameters such as `do_sample`, `top_k`, and `top_p`, which adjust the sampling strategy for the next-word prediction to create varied or focused summaries. The time taken for the summarization process is measured, and both the generated summary and the time duration are printed. This approach provides a concise summary of the document in a controlled and optimized manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba3f5aa-55ab-4421-9a46-e214e2727259",
      "metadata": {
        "id": "dba3f5aa-55ab-4421-9a46-e214e2727259"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "# Load a summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "\n",
        "document = text\n",
        "\n",
        "# Generate the summary\n",
        "# do_sample: When set to True, the model samples the next word from the probability distribution, potentially leading to more varied summaries.\n",
        "# Setting do_sample to False uses greedy decoding, which may be more accurate for some contexts.\n",
        "# top_k: Limits the number of next-token predictions. A smaller top_k will make the output more focused.\n",
        "# top_p: Nucleus sampling, which allows the model to consider a smaller number of potential next tokens. Lowering top_p can lead to a more concise summary.\n",
        "t1 = time.time()\n",
        "summary = summarizer(text, max_length=150, min_length=50, do_sample=True, top_k=50, top_p=0.9)\n",
        "t2 = time.time()\n",
        "t3 = t2 - t1\n",
        "\n",
        "# Print the summary\n",
        "print(summary[0]['summary_text'])\n",
        "print(\"Summarization took in seconds: \", t3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8893a30-14c5-46db-840e-d265fe0e0fe8",
      "metadata": {
        "id": "b8893a30-14c5-46db-840e-d265fe0e0fe8"
      },
      "source": [
        "## Create frames from the video\n",
        "This code extracts frames from a video at a rate of 1 frame per second and saves each frame as a PNG image. The process begins by capturing the current time, then loops through the video duration, extracting frames at each second and saving them with filenames like frame_0.png, frame_1.png, etc. After the extraction, it calculates and prints the total time taken for the process. This allows for easy extraction and saving of video frames for further processing or analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8150426-5263-42a5-ace2-cf4de204ccc9",
      "metadata": {
        "id": "b8150426-5263-42a5-ace2-cf4de204ccc9"
      },
      "outputs": [],
      "source": [
        "# Optionally, you can save or further process the extracted frames\n",
        "# Extracting and saving frames as images\n",
        "import time\n",
        "t1 = time.time()\n",
        "frame_rate = 1  # 1 frame per second\n",
        "for t in range(0, int(video.duration), frame_rate):\n",
        "    frame = video.get_frame(t)\n",
        "    frame_image = mp.ImageClip(frame)\n",
        "    frame_image.save_frame(f\"frame_{t}.png\")\n",
        "\n",
        "t2 = time.time()\n",
        "t3 = t2 - t1\n",
        "print(\"total time: \",t3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fc35a98-4bda-4d0e-a9f4-8143549f8880",
      "metadata": {
        "id": "4fc35a98-4bda-4d0e-a9f4-8143549f8880"
      },
      "source": [
        "## Copy Frames in a directory\n",
        "This code manages the setup of a directory for storing extracted video frames. It defines the directory path as `\"frames\"` and checks if this directory already exists. If it does, the directory and its contents are removed. After that, a new `frames` directory is created using the `mkdir -p` command. Finally, all files matching the pattern `frame*.*` (such as extracted frame images) are copied into the `frames` directory for organized storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5c20551-fd6b-47f5-adc2-ef58ba1271ac",
      "metadata": {
        "id": "c5c20551-fd6b-47f5-adc2-ef58ba1271ac"
      },
      "outputs": [],
      "source": [
        "# Define the directory path\n",
        "directory_path = \"frames\"\n",
        "\n",
        "# Execute the shell commands\n",
        "!if [ -d \"{directory_path}\" ]; then sudo rm -rf \"{directory_path}\"; fi\n",
        "!sudo mkdir -p \"{directory_path}\"\n",
        "!sudo cp -r frame*.* ./frames"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d5ff67-e485-445f-8e7f-281a069438a1",
      "metadata": {
        "id": "79d5ff67-e485-445f-8e7f-281a069438a1"
      },
      "source": [
        "## Optimization with OpenVINO\n",
        "In the above cell, the step refine_response_with_gpt_neo took 85.75 seconds, which is fairly long. But we can optimiza the model inference with OpenVINO. We will quantize the model and use OpenVINO to use the quantized model, which in turn will use the AMX (Advanced Matrix Instruction) within the Xeon core. This will significantly reduce the LLM inference time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08506923-7597-484a-b2cd-614749264f7a",
      "metadata": {
        "id": "08506923-7597-484a-b2cd-614749264f7a"
      },
      "source": [
        "## Install Required Libraries\n",
        "In this step we install the required libraries.\n",
        "\n",
        "**Optimum[Openvino]**\n",
        "Optimum OpenVINO is a toolkit designed to optimize and accelerate deep learning models for inference, specifically targeting Intel hardware platforms. It integrates with Hugging Face's Optimum library, providing streamlined support for running and fine-tuning popular models like transformers, vision models, and diffusion models on Intel's CPUs, GPUs, and other AI accelerators. Optimum OpenVINO enhances performance by enabling model quantization, pruning, and other optimizations, making it easier for developers to deploy efficient AI solutions on Intel hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "133f37b3-09a6-4aa8-abe0-701381025e50",
      "metadata": {
        "scrolled": true,
        "id": "133f37b3-09a6-4aa8-abe0-701381025e50"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade-strategy eager optimum[openvino,nncf]\n",
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df37ec10-3933-499f-ad8c-9ad1be503350",
      "metadata": {
        "id": "df37ec10-3933-499f-ad8c-9ad1be503350"
      },
      "source": [
        "## Quantize the model with OpenVINO\n",
        "Now we will use OpenVino to quantize the model.\n",
        "\n",
        "This code sets up and exports a pre-trained language model for efficient inference using **OpenVINO**. It configures the OpenVINO runtime to use 32 logical CPU cores and enable **AMX** instructions (for BF16 and INT8 operations) to optimize performance. In this case, our Xeon server has 32 logical cores. The model, **GPT-Neo 2.7B**, is loaded with specific quantization settings, which reduce the model size by using **bf16** weights. The model is then exported to the **OpenVINO IR** format and saved in a specified directory for future inference tasks, ensuring efficient and optimized deployment.\n",
        "\n",
        "***In this step, I ran into the error `No module named 'transformers.models.mamba2.configuration_mamba2'`. If you run into the same problem, I had to restart the Jupyter notebook kernel and that solved the issue.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bef0c0f-7e12-4f78-ba26-2055d76f723f",
      "metadata": {
        "id": "6bef0c0f-7e12-4f78-ba26-2055d76f723f"
      },
      "outputs": [],
      "source": [
        "from optimum.intel import OVModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "import openvino.runtime as ov\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "output_dir = \"./converted_model\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Set OpenVINO runtime to use 32 logical cores and enable AMX instructions\n",
        "core = ov.Core()\n",
        "core.set_property(device_name=\"CPU\", properties={\n",
        "    \"CPU_THREADS_NUM\": \"32\",\n",
        "    \"CPU_ENABLE_AMX_BF16\": \"YES\",  # Enable AMX BF16 instructions\n",
        "    \"CPU_ENABLE_AMX_INT8\": \"YES\"   # Enable AMX INT8 instructions\n",
        "})\n",
        "\n",
        "# Set the OpenVINO logging level to DEBUG to see detailed logs\n",
        "os.environ[\"OV_LOG_LEVEL\"] = \"LOG_DEBUG\"  # or LOG_TRACE for even more detail\n",
        "\n",
        "\n",
        "# Define the quantization configuration as a dictionary\n",
        "quantization_config = {\n",
        "    \"algorithm\": \"DefaultQuantization\", # Specify the quantization algorithm\n",
        "    \"weight_type\": \"bf16\"               # Specify the weight type (e.g., int8, int4)\n",
        "}\n",
        "\n",
        "# load the model and export it to OpenVINO IR format\n",
        "model_id = \"EleutherAI/gpt-neo-2.7B\"\n",
        "model = OVModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    export=True,\n",
        "    save_dir=output_dir,\n",
        "    quantization=quantization_config,  # Pass the quantization config here\n",
        "    ov_core=core  # Pass the OpenVINO core with the specified thread settings\n",
        ")\n",
        "\n",
        "# Save the model in the IR format\n",
        "model.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4082c33-b8e8-4730-b194-535cd83fb2b1",
      "metadata": {
        "id": "e4082c33-b8e8-4730-b194-535cd83fb2b1"
      },
      "source": [
        "## Query the images from the video and generate response - this time optimized with OpenVINO\n",
        "This code processes frames from a video to answer visual questions using a combination of a **Visual Question Answering (VQA)** model and an optimized **GPT-Neo** language model with **OpenVINO** for efficient inference. It first checks if a **GPU** is available, then initializes the VQA model to query frames based on a given question. The frames are loaded from a directory and processed through the VQA model to generate answers. These answers are combined into a cohesive response, which is further refined using the **OpenVINO**-optimized GPT-Neo model to produce a more natural and coherent text output. The final response is displayed, providing an accurate and structured answer, while performance is optimized through the **OpenVINO** runtime. Timing for each step is recorded for performance analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c84665-4064-4b34-9690-8303299d8d1f",
      "metadata": {
        "id": "b8c84665-4064-4b34-9690-8303299d8d1f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from PIL import Image\n",
        "from transformers import ViltProcessor, ViltForQuestionAnswering, GPTNeoForCausalLM, GPT2Tokenizer\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU for computation\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA is not available, using CPU for computation\")\n",
        "\n",
        "# Initialize the VQA model and processor\n",
        "model_name = \"dandelin/vilt-b32-finetuned-vqa\"\n",
        "processor = ViltProcessor.from_pretrained(model_name)\n",
        "model = ViltForQuestionAnswering.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Initialize the GPT-Neo model and tokenizer\n",
        "gpt_neo_tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "\n",
        "# Load the OpenVINO model from the saved directory\n",
        "converted_model = OVModelForCausalLM.from_pretrained(output_dir, ov_core=core).to(device)\n",
        "\n",
        "# Directory containing PNG files\n",
        "frames_dir = '/home/ubuntu/frames'\n",
        "\n",
        "# Load frames\n",
        "def load_frames():\n",
        "    frames = {}\n",
        "    for frame_file in os.listdir(frames_dir):\n",
        "        if frame_file.endswith('.png'):\n",
        "            img_path = os.path.join(frames_dir, frame_file)\n",
        "            img = Image.open(img_path)\n",
        "            frames[frame_file] = img\n",
        "    return frames\n",
        "\n",
        "frames = load_frames()\n",
        "\n",
        "# Function to query frames\n",
        "def query_frames(query):\n",
        "    results = []\n",
        "    for frame_file, img in frames.items():\n",
        "        inputs = processor(text=query, images=img, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs.to(device))\n",
        "\n",
        "        answer_idx = outputs.logits.argmax(-1).item()\n",
        "        answer = model.config.id2label[answer_idx] if answer_idx in model.config.id2label else f\"[unused{answer_idx}]\"\n",
        "\n",
        "        results.append(answer)\n",
        "    return results\n",
        "\n",
        "# Combine answers into a meaningful response\n",
        "def combine_answers(query, answers):\n",
        "    unique_answers = set(answers)\n",
        "    if query.lower().startswith(\"how many\"):\n",
        "        num_counts = {}\n",
        "        for answer in unique_answers:\n",
        "            num_counts[answer] = answers.count(answer)\n",
        "        num_answer = \" and \".join([f\"{count} {ans}\" for ans, count in num_counts.items()])\n",
        "        combined_response = f\"There are {num_answer} in the video.\"\n",
        "    else:\n",
        "        combined_response = \" \".join(unique_answers)\n",
        "\n",
        "    return combined_response\n",
        "\n",
        "# Use GPT-Neo to refine the response\n",
        "def refine_response_with_gpt_neo(question, combined_response):\n",
        "    prompt = f\"Question: {question}\\nCombined Response: {combined_response}\\nProvide a single, coherent English answer based on the combined response:\"\n",
        "\n",
        "    inputs = gpt_neo_tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = converted_model.generate(\n",
        "        inputs,\n",
        "        max_length=100,\n",
        "        min_length=50,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=gpt_neo_tokenizer.eos_token_id,\n",
        "        eos_token_id=gpt_neo_tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=2  # Ensure the model does not repeat phrases\n",
        "    )\n",
        "\n",
        "    generated_text = gpt_neo_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the answer from the generated text\n",
        "    refined_answer = generated_text.split('Provide a single, coherent English answer based on the combined response:')[-1].strip()\n",
        "\n",
        "    return refined_answer\n",
        "\n",
        "# Example query\n",
        "query = \"What was the seargent wearing who was giving the speech?\"\n",
        "\n",
        "t1 = time.time()\n",
        "answers = query_frames(query)\n",
        "t2 = time.time()\n",
        "t3 = t2 - t1\n",
        "print(\"query_frames time: \", t3)\n",
        "\n",
        "\n",
        "t1 = time.time()\n",
        "combined_response = combine_answers(query, answers)\n",
        "t2 = time.time()\n",
        "t3 = t2 - t1\n",
        "print(\"combine_answers time: \", t3)\n",
        "\n",
        "t1 = time.time()\n",
        "final_response = refine_response_with_gpt_neo(query, combined_response)\n",
        "t2 = time.time()\n",
        "t3 = t2 - t1\n",
        "print(\"refine_response_with_gpt_neo time: \", t3)\n",
        "\n",
        "# Display results\n",
        "print(\"Final Response:\", final_response.split('.')[0].strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe3c9dc8-6d5f-49a0-95be-83a97f187ccf",
      "metadata": {
        "id": "fe3c9dc8-6d5f-49a0-95be-83a97f187ccf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
