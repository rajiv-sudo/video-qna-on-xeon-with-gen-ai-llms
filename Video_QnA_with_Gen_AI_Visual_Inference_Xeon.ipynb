{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "073fd077-2cad-449f-9268-48417e69ee18",
      "metadata": {
        "id": "073fd077-2cad-449f-9268-48417e69ee18"
      },
      "source": [
        "# Description:\n",
        "This notebook focuses on video analysis using Generative AI. It begins by extracting audio from a video, transcribing it into text, and then applying AI techniques to query or summarize the transcription, offering quick insights. The video is also broken down into frames at a configurable rate, allowing for flexible image generation. These frames are processed for visual question-answering, with redundant information removed, and a combined response is generated using a large language model (LLM). This approach enables effective interaction with video content to extract meaningful insights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbf5485b-2f34-4caa-a5b0-909cad451183",
      "metadata": {
        "id": "bbf5485b-2f34-4caa-a5b0-909cad451183"
      },
      "source": [
        "## Install necessary libraries\n",
        "These instructions are for setting up a Python environment with various libraries and tools, mainly for handling video processing, machine learning, and serving AI models. Here's a breakdown:\n",
        "\n",
        "- **opencv-python**: Installs OpenCV, a library for computer vision tasks.\n",
        "- **moviepy**: Installs MoviePy, a library for video editing.\n",
        "- **ffmpeg**: Installs FFmpeg, a tool for handling multimedia files.\n",
        "- **transformers**: Installs Hugging Face's Transformers library for working with transformer-based models.\n",
        "- **torch, torchvision, torchaudio**: Installs PyTorch and related libraries for machine learning tasks with images (torchvision) and audio (torchaudio).\n",
        "- **accelerate**: Installs Accelerate, a library for optimizing and distributing model training.\n",
        "- **pillow, flask**: Installs Pillow (image handling) and Flask (for creating web applications).\n",
        "- **txtai**: Installs the txtai library (with API and pipeline options), which enables AI-powered search and embeddings.\n",
        "\n",
        "These are used to set up an environment for video, image, and text processing with machine learning capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da139805-4e3f-4604-8467-9499d5aa7ea3",
      "metadata": {
        "scrolled": true,
        "id": "da139805-4e3f-4604-8467-9499d5aa7ea3"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python\n",
        "!pip3 install moviepy\n",
        "!sudo apt install ffmpeg -y\n",
        "!pip3 install transformers\n",
        "!pip3 install torch torchvision torchaudio\n",
        "!pip3 install accelerate -y\n",
        "!pip3 install torch pillow flask\n",
        "!pip install git+https://github.com/neuml/txtai#egg=txtai[api,pipeline]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1023fc6b-db60-4a53-aea6-58057e1c6594",
      "metadata": {
        "id": "1023fc6b-db60-4a53-aea6-58057e1c6594"
      },
      "source": [
        "## Hugging Face Login\n",
        "This code imports the `notebook_login` function from the Hugging Face Hub and calls it to prompt the user to enter their Hugging Face token. This token is necessary to authenticate the user and gain access to Hugging Face's resources, such as models and datasets, from within a Jupyter Notebook or similar environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f6c4bbc-d429-4f46-bafe-c43c28e7e7c2",
      "metadata": {
        "id": "4f6c4bbc-d429-4f46-bafe-c43c28e7e7c2"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# This will prompt you to enter your Hugging Face token\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a10c728a-5a80-4a20-997c-e1c3cd51b397",
      "metadata": {
        "id": "a10c728a-5a80-4a20-997c-e1c3cd51b397"
      },
      "source": [
        "## TXTAI directory setup\n",
        "This code sets up a directory named `txtai` by first ensuring that any pre-existing directory with that name is removed. It starts by defining the path to the directory as `\"txtai\"`, and then runs a shell command to check if the directory already exists. If it does, it deletes the directory along with all of its contents using the `sudo rm -rf` command. After clearing any previous data, the code creates a new directory named `txtai` using the `sudo mkdir -p` command, which ensures the directory is created without any errors, even if it already exists. This guarantees that a fresh, empty directory is available for use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2df324c-3885-4e16-aa85-a3cf14554231",
      "metadata": {
        "id": "b2df324c-3885-4e16-aa85-a3cf14554231"
      },
      "outputs": [],
      "source": [
        "# Define the directory path\n",
        "directory_path = \"txtai\"\n",
        "\n",
        "# Execute the shell commands\n",
        "!if [ -d \"{directory_path}\" ]; then sudo rm -rf \"{directory_path}\"; fi\n",
        "!sudo mkdir -p \"{directory_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eed0f54-ae08-455a-8bce-c9dd0ad34ed3",
      "metadata": {
        "id": "7eed0f54-ae08-455a-8bce-c9dd0ad34ed3"
      },
      "source": [
        "## TXTAI Pipeline\n",
        "This code initializes a transcription model by importing the `Transcription` class from the `txtai.pipeline` module, which is designed to convert audio into text. By calling `Transcription()`, it creates an instance named `transcribe`, which can now be used to handle transcription tasks. This allows the user to input audio files and convert spoken language into written text automatically. In summary, the code sets up a transcription model from the `txtai` library, enabling efficient audio-to-text conversion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c5a1899-8f2d-4ea6-88ab-440fbe4485be",
      "metadata": {
        "id": "3c5a1899-8f2d-4ea6-88ab-440fbe4485be"
      },
      "outputs": [],
      "source": [
        "from txtai.pipeline import Transcription\n",
        "\n",
        "# Create transcription model\n",
        "transcribe = Transcription()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1db38744-fdfa-46c2-b1ab-edc800c00716",
      "metadata": {
        "id": "1db38744-fdfa-46c2-b1ab-edc800c00716"
      },
      "source": [
        "\n",
        "## Generate audio from video\n",
        "This code handles video and audio processing by using the `moviepy.editor` module. It starts by importing the library as `mp` for video editing tasks. Next, it loads a video file called `your_video.mp4` into a `VideoFileClip` object, making it accessible for further manipulation. The code then extracts the audio track from the video, assigning the output to a file named `your_video.wav`. After successfully extracting and saving the audio, a shell command is executed to copy the `your_video.wav` file into the `txtai` directory using `sudo cp`. In summary, this code processes a video by extracting its audio, saving it in `.wav` format, and moving the audio file to a designated directory for further use.\n",
        "\n",
        "**Replace all references of `your_video` or `your_video.mp4` or `your_video.wav` with the actual name of your video file or video clip.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a254a27-8643-498a-a6ce-2a9f70a76a01",
      "metadata": {
        "id": "2a254a27-8643-498a-a6ce-2a9f70a76a01"
      },
      "outputs": [],
      "source": [
        "import moviepy.editor as mp\n",
        "\n",
        "# Load the video file\n",
        "video = mp.VideoFileClip(\"your_video.mp4\")\n",
        "\n",
        "# Extract audio from the video\n",
        "audio_path = \"your_video.wav\"\n",
        "video.audio.write_audiofile(audio_path)\n",
        "\n",
        "!sudo cp your_video.wav ./txtai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc11951-66ce-49b9-afc3-b6def5e6808f",
      "metadata": {
        "id": "cdc11951-66ce-49b9-afc3-b6def5e6808f"
      },
      "source": [
        "## List of audio files to process\n",
        "This code sets up the necessary imports and file list to display and play audio files within a Jupyter notebook. First, it imports the `Audio` and `display` functions from the `IPython.display` module, which are used to handle and showcase audio files. Then, it creates a list named `files` containing the file name `your_video.wav`. The second line modifies this list by prepending the path `txtai/` to each file name, resulting in a full file path like `txtai/your_video.wav`. In summary, this code prepares audio files for playback from the `txtai` directory and allows for easy extension by adding more audio files to the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f11e5c31-297f-4c9a-bbb3-b4f38642a96e",
      "metadata": {
        "id": "f11e5c31-297f-4c9a-bbb3-b4f38642a96e"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio, display\n",
        "\n",
        "files = [\"your_video.wav\"] # Specify here for multiple file\n",
        "files = [\"txtai/%s\" % x for x in files]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "865daba8-9225-4f73-9e8a-e29f1a3296e4",
      "metadata": {
        "id": "865daba8-9225-4f73-9e8a-e29f1a3296e4"
      },
      "source": [
        "## Transcription\n",
        "\n",
        "\n",
        "This code initializes a transcription model using OpenAI's Whisper model, specifically the `\"openai/whisper-base\"` version, which is designed for automatic speech recognition (ASR) tasks. The model, `transcribe`, is used to process audio files listed in the `files` variable (e.g., `\"txtai/your_video.wav\"`). The code iterates over the transcription results, where the `transcribe(files)` function processes each audio file and converts spoken content into text. Finally, the transcriptions are printed to the console using `print(text)`. In summary, this code leverages the Whisper model to transcribe audio files and display the transcribed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83de1470-edca-478b-9995-2047f0fc14cf",
      "metadata": {
        "id": "83de1470-edca-478b-9995-2047f0fc14cf"
      },
      "outputs": [],
      "source": [
        "# Transcribe files\n",
        "transcribe = Transcription(\"openai/whisper-base\")\n",
        "for text in transcribe(files):\n",
        "  print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d188f2b4-34ea-425d-b449-049560a4605e",
      "metadata": {
        "id": "d188f2b4-34ea-425d-b449-049560a4605e"
      },
      "source": [
        "## Ingestion directory setup\n",
        "This code below sets up the directory for document ingestion into a vector database. It first defines the directory path as `/home/ubuntu/documents` and checks if the directory already exists. If it does, the directory and its contents are deleted using the `rm -rf` command. Afterward, a new directory is created at the same location using the `mkdir -p` command. This ensures a clean directory structure is prepared for ingesting documents into the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b406d8f6-cab4-4467-9cb7-e16f708be1d9",
      "metadata": {
        "id": "b406d8f6-cab4-4467-9cb7-e16f708be1d9"
      },
      "outputs": [],
      "source": [
        "# Define the directory path for ingestion of documents into vector db\n",
        "ingestion_path = \"/home/ubuntu/documents\"\n",
        "\n",
        "# Execute the shell commands\n",
        "!if [ -d \"{ingestion_path}\" ]; then sudo rm -rf \"{ingestion_path}\"; fi\n",
        "!sudo mkdir -p \"{ingestion_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd92fc65-b6ae-4a96-aeb1-8d71970d6d7f",
      "metadata": {
        "id": "bd92fc65-b6ae-4a96-aeb1-8d71970d6d7f"
      },
      "source": [
        "## Chat with transcribed text using Gen AI LLM\n",
        "This code uses a pre-trained question-answering model from the `transformers` library to answer questions about video content. The `ask_question` function takes a question and uses the `qa_pipeline` to generate an answer based on a provided context (the transcribed video text). An example question, \"What happened to the 17 year old boy?\", is passed to the function, and the answer is printed. This allows users to interactively ask questions about a video using natural language processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eff6688-dc9c-4f6a-93a1-fa4adb416c5f",
      "metadata": {
        "id": "9eff6688-dc9c-4f6a-93a1-fa4adb416c5f"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "# Load a question-answering pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", device=-1)\n",
        "\n",
        "# Function to ask questions about the video\n",
        "def ask_question(question):\n",
        "    context = text\n",
        "    result = qa_pipeline(question=question, context=context, max_tokens=100)\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "question = \"What happened to the 17 year old boy?\"\n",
        "t1 = time.time()\n",
        "answer = ask_question(question)\n",
        "t2 = time.time()\n",
        "t3 = t2 - t1\n",
        "\n",
        "print(\"QnA from transcribed text takes: \", t3, \" seconds\")\n",
        "print(f\"Question: {question}\\nAnswer: {answer['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14684a7f-2e92-466e-8a65-74787446a5f2",
      "metadata": {
        "id": "14684a7f-2e92-466e-8a65-74787446a5f2"
      },
      "source": [
        "## Summarize transcribed text using Gen AI LLM\n",
        "This code utilizes the `transformers` library to generate a summary of a given document using a pre-trained summarization model, specifically `\"facebook/bart-large-cnn\"`. The summarization process is controlled by parameters such as `do_sample`, `top_k`, and `top_p`, which adjust the sampling strategy for the next-word prediction to create varied or focused summaries. The time taken for the summarization process is measured, and both the generated summary and the time duration are printed. This approach provides a concise summary of the document in a controlled and optimized manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba3f5aa-55ab-4421-9a46-e214e2727259",
      "metadata": {
        "id": "dba3f5aa-55ab-4421-9a46-e214e2727259"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "# Load a summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "\n",
        "document = text\n",
        "\n",
        "# Generate the summary\n",
        "# do_sample: When set to True, the model samples the next word from the probability distribution, potentially leading to more varied summaries.\n",
        "# Setting do_sample to False uses greedy decoding, which may be more accurate for some contexts.\n",
        "# top_k: Limits the number of next-token predictions. A smaller top_k will make the output more focused.\n",
        "# top_p: Nucleus sampling, which allows the model to consider a smaller number of potential next tokens. Lowering top_p can lead to a more concise summary.\n",
        "t1 = time.time()\n",
        "summary = summarizer(text, max_length=150, min_length=50, do_sample=True, top_k=50, top_p=0.9)\n",
        "t2 = time.time()\n",
        "t3 = t2 - t1\n",
        "\n",
        "# Print the summary\n",
        "print(summary[0]['summary_text'])\n",
        "print(\"Summarization took in seconds: \", t3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8893a30-14c5-46db-840e-d265fe0e0fe8",
      "metadata": {
        "id": "b8893a30-14c5-46db-840e-d265fe0e0fe8"
      },
      "source": [
        "## Create frames from the video\n",
        "This code extracts frames from a video at a rate of 1 frame per second and saves each frame as a PNG image. The process begins by capturing the current time, then loops through the video duration, extracting frames at each second and saving them with filenames like frame_0.png, frame_1.png, etc. After the extraction, it calculates and prints the total time taken for the process. This allows for easy extraction and saving of video frames for further processing or analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8150426-5263-42a5-ace2-cf4de204ccc9",
      "metadata": {
        "id": "b8150426-5263-42a5-ace2-cf4de204ccc9"
      },
      "outputs": [],
      "source": [
        "# Optionally, you can save or further process the extracted frames\n",
        "# Extracting and saving frames as images\n",
        "import time\n",
        "t1 = time.time()\n",
        "frame_rate = 1  # 1 frame per second\n",
        "for t in range(0, int(video.duration), frame_rate):\n",
        "    frame = video.get_frame(t)\n",
        "    frame_image = mp.ImageClip(frame)\n",
        "    frame_image.save_frame(f\"frame_{t}.png\")\n",
        "\n",
        "t2 = time.time()\n",
        "t3 = t2 - t1\n",
        "print(\"total time: \",t3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fc35a98-4bda-4d0e-a9f4-8143549f8880",
      "metadata": {
        "id": "4fc35a98-4bda-4d0e-a9f4-8143549f8880"
      },
      "source": [
        "## Copy Frames in a directory\n",
        "This code manages the setup of a directory for storing extracted video frames. It defines the directory path as `\"frames\"` and checks if this directory already exists. If it does, the directory and its contents are removed. After that, a new `frames` directory is created using the `mkdir -p` command. Finally, all files matching the pattern `frame*.*` (such as extracted frame images) are copied into the `frames` directory for organized storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5c20551-fd6b-47f5-adc2-ef58ba1271ac",
      "metadata": {
        "id": "c5c20551-fd6b-47f5-adc2-ef58ba1271ac"
      },
      "outputs": [],
      "source": [
        "# Define the directory path\n",
        "directory_path = \"frames\"\n",
        "\n",
        "# Execute the shell commands\n",
        "!if [ -d \"{directory_path}\" ]; then sudo rm -rf \"{directory_path}\"; fi\n",
        "!sudo mkdir -p \"{directory_path}\"\n",
        "!sudo cp -r frame*.* ./frames"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00d5f9d2-ead1-413c-bd35-9bd6f136a6ce",
      "metadata": {
        "id": "00d5f9d2-ead1-413c-bd35-9bd6f136a6ce"
      },
      "source": [
        "## Query the images from the video and generate response\n",
        "### Code Explanation with Main Points Highlighted:\n",
        "\n",
        "1. **Check for CUDA Availability**:\n",
        "   - The code first checks if **CUDA (GPU)** is available for faster computation. If CUDA is available, it uses the GPU for processing, otherwise, it falls back to the **CPU**. This ensures efficient use of resources based on the hardware.\n",
        "\n",
        "2. **Initialize VQA Model and Processor**:\n",
        "   - It sets up a **Visual Question Answering (VQA)** model and processor from Hugging Face, specifically the **\"dandelin/vilt-b32-finetuned-vqa\"** model. The model is designed to process both text (questions) and images (frames) to answer visual questions.\n",
        "\n",
        "3. **Initialize GPT-Neo Model and Tokenizer**:\n",
        "   - The code also initializes the **GPT-Neo language model**, which is a powerful text-generation model. The tokenizer is used to convert text inputs into a format that the model can process. This model is later used to refine the answers produced by the VQA model into more fluent and coherent responses.\n",
        "\n",
        "4. **Load Frames**:\n",
        "   - It defines a function to load **image frames** (in PNG format) from a specified directory. The frames are stored in memory and used for querying. Each frame represents a snapshot from the video, and these are processed later to extract relevant information.\n",
        "\n",
        "5. **Query Frames**:\n",
        "   - For each frame, the **VQA model** is used to answer a given question. The model processes both the text of the question and the visual content of each frame to generate a relevant answer for each frame. These answers are collected for further processing.\n",
        "\n",
        "6. **Combine Answers**:\n",
        "   - Once the answers are collected from the frames, they are combined into a **single coherent response**. The function handles specific types of questions, such as those asking for counts (\"How many\"), and produces a logical summary of the answers provided by the VQA model.\n",
        "\n",
        "7. **Refine Response with GPT-Neo**:\n",
        "   - After combining the answers, the **GPT-Neo model** refines the response to make it more natural and coherent. The GPT-Neo model takes the raw combined response and generates a well-structured, fluent answer in plain English.\n",
        "\n",
        "8. **Timing and Execution**:\n",
        "   - The code also tracks how long each major step (querying frames, combining answers, and refining the response) takes. This is useful for performance analysis and optimization, providing insight into the time complexity of each process.\n",
        "\n",
        "9. **Final Response**:\n",
        "   - After all the steps are completed, the final, refined response is printed. This answer is the system's conclusion, based on the question and the information extracted from the video frames, providing a seamless interaction between visual and textual data processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf98474-42cf-4ec4-8a5d-2ca2bd25f9cc",
      "metadata": {
        "id": "5cf98474-42cf-4ec4-8a5d-2ca2bd25f9cc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from transformers import ViltProcessor, ViltForQuestionAnswering, GPTNeoForCausalLM, GPT2Tokenizer\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU for computation\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA is not available, using CPU for computation\")\n",
        "\n",
        "# Initialize the VQA model and processor\n",
        "model_name = \"dandelin/vilt-b32-finetuned-vqa\"\n",
        "processor = ViltProcessor.from_pretrained(model_name)\n",
        "model = ViltForQuestionAnswering.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Initialize the GPT-Neo model and tokenizer\n",
        "gpt_neo_tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "gpt_neo_model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\").to(device)\n",
        "\n",
        "# Directory containing PNG files\n",
        "frames_dir = '/home/ubuntu/frames'\n",
        "\n",
        "# Load frames\n",
        "def load_frames():\n",
        "    frames = {}\n",
        "    for frame_file in os.listdir(frames_dir):\n",
        "        if frame_file.endswith('.png'):\n",
        "            img_path = os.path.join(frames_dir, frame_file)\n",
        "            img = Image.open(img_path)\n",
        "            frames[frame_file] = img\n",
        "    return frames\n",
        "\n",
        "frames = load_frames()\n",
        "\n",
        "# Function to query frames\n",
        "def query_frames(query):\n",
        "    results = []\n",
        "    for frame_file, img in frames.items():\n",
        "        inputs = processor(text=query, images=img, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs.to(device))\n",
        "\n",
        "        answer_idx = outputs.logits.argmax(-1).item()\n",
        "        answer = model.config.id2label[answer_idx] if answer_idx in model.config.id2label else f\"[unused{answer_idx}]\"\n",
        "\n",
        "        results.append(answer)\n",
        "    return results\n",
        "\n",
        "# Combine answers into a meaningful response\n",
        "def combine_answers(query, answers):\n",
        "    unique_answers = set(answers)\n",
        "    if query.lower().startswith(\"how many\"):\n",
        "        num_counts = {}\n",
        "        for answer in unique_answers:\n",
        "            num_counts[answer] = answers.count(answer)\n",
        "        num_answer = \" and \".join([f\"{count} {ans}\" for ans, count in num_counts.items()])\n",
        "        combined_response = f\"There are {num_answer} in the video.\"\n",
        "    else:\n",
        "        combined_response = \" \".join(unique_answers)\n",
        "\n",
        "    return combined_response\n",
        "\n",
        "# Use GPT-Neo to refine the response\n",
        "def refine_response_with_gpt_neo(question, combined_response):\n",
        "    prompt = f\"Question: {question}\\nCombined Response: {combined_response}\\nProvide a single, coherent English answer based on the combined response:\"\n",
        "\n",
        "    inputs = gpt_neo_tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = gpt_neo_model.generate(\n",
        "        inputs,\n",
        "        max_length=100,\n",
        "        min_length=50,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=gpt_neo_tokenizer.eos_token_id,\n",
        "        eos_token_id=gpt_neo_tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=2  # Ensure the model does not repeat phrases\n",
        "    )\n",
        "\n",
        "    generated_text = gpt_neo_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the answer from the generated text\n",
        "    refined_answer = generated_text.split('Provide a single, coherent English answer based on the combined response:')[-1].strip()\n",
        "\n",
        "    return refined_answer\n",
        "\n",
        "# Example query\n",
        "query = \"What was the seargent wearing who was giving the speech?\"\n",
        "\n",
        "t1 = time.time()\n",
        "answers = query_frames(query)\n",
        "t2 = time.time()\n",
        "t3 = t2 - t1\n",
        "print(\"query_frames time: \", t3)\n",
        "\n",
        "\n",
        "t1 = time.time()\n",
        "combined_response = combine_answers(query, answers)\n",
        "t2 = time.time()\n",
        "t3 = t2 - t1\n",
        "print(\"combine_answers time: \", t3)\n",
        "\n",
        "t1 = time.time()\n",
        "final_response = refine_response_with_gpt_neo(query, combined_response)\n",
        "t2 = time.time()\n",
        "t3 = t2 - t1\n",
        "print(\"refine_response_with_gpt_neo time: \", t3)\n",
        "\n",
        "# Display results\n",
        "print(\"Final Response:\", final_response.split('.')[0].strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "269e601d-38be-4d91-8952-aa18dd35c04a",
      "metadata": {
        "id": "269e601d-38be-4d91-8952-aa18dd35c04a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
